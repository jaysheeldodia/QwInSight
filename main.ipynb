{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a123e2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40859d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virtual-viking/projects/doc-qa/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111f5eb",
   "metadata": {},
   "source": [
    "### Loading PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f044fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\"./paper.pdf\")\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "context = docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9b2f2",
   "metadata": {},
   "source": [
    "### Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd5aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99c4219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93be6415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210220b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c3e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56973102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0719ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4295dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19af9950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user is asking for a short introduction to large language models. Let me start by recalling what I know about them. Large language models are AI systems that process and generate human-like text. They're based on deep learning, specifically transformer architecture.\n",
      "\n",
      "I should mention their key components, like neural networks and training on vast amounts of text data. Maybe explain how they understand context and generate coherent responses. Also, highlight their applications in various fields such as chatbots, content creation, coding, etc.\n",
      "\n",
      "Wait, the user wants it to be short, so I need to be concise. Avoid technical jargon but still be accurate. Maybe start with a definition, then key features, and then applications. Also, note that they're trained on massive datasets, which allows them to handle complex tasks. Don't forget to mention that they can be fine-tuned for specific uses. Let me check if I'm missing anything important. Oh, right, they use self-attention mechanisms for context understanding. That's a key point. Alright, structure it in a way that's easy to follow without being too lengthy.\n",
      "</think>\n",
      "content: Large language models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. Based on deep learning and transformer architecture, they process vast amounts of textual data to recognize patterns, context, and meaning. These models can perform tasks like answering questions, writing essays, coding, translating languages, and engaging in conversations. Trained on massive datasets, LLMs excel at generating coherent, contextually relevant responses, making them powerful tools across industries, from healthcare to entertainment. Their ability to adapt through fine-tuning further enhances their versatility.\n"
     ]
    }
   ],
   "source": [
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da2353",
   "metadata": {},
   "source": [
    "### Prompt + Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e5d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/virtual-viking/projects/doc-qa/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaba5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the document\n",
    "file_path = \"./paper.pdf\"\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Extract context from the PDF (first page, for simplicity)\n",
    "context = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b70d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  4.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the model and tokenizer (as per Hugging Face recommended method)\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb36b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the question and the prompt\n",
    "question = \"Who are the authors of this paper?\"\n",
    "prompt = f\"\"\"Given the following context from the pdf:\n",
    "{context}\n",
    "\n",
    "Answer the following question: \"{question}\"\n",
    "\n",
    "Please answer the following question based on the context. Do not add any explanations or extra information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "456ad23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Tokenize the input and prepare the model input\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Use the tokenizer's apply_chat_template method for formatting\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False  # Optional: controls thinking mode, default is True\n",
    ")\n",
    "\n",
    "# Tokenize the input text\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b6a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Generate the response\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=100  # Adjust based on your needs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeaf058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract the generated content\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# Parse thinking content\n",
    "try:\n",
    "    # Look for the thinking token (e.g., </think>) in the output\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b41be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking content: \n",
      "Answer: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Print the result\n",
    "print(\"Thinking content:\", thinking_content)\n",
    "print(\"Answer:\", content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
